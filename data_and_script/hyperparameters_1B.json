{"batch_size": 8, "epochs": 80, "l1_dropout": 0.0, "l1_reg": 0.0037643230749325103, "l2_dropout": 0.0, "l2_reg": 0.0001686075599065116, "layers": 2, "learning_rate": 0.0008005503377845339, "n_neurons_l1": 4, "n_neurons_l2": 8}
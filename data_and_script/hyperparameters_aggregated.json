{"batch_size": 16, "epochs": 80, "l1_dropout": 0.0, "l1_reg": 0.0008105513512125289, "l2_dropout": 0.0, "l2_reg": 0.00024200993182043573, "layers": 2, "learning_rate": 0.006035318907544431, "n_neurons_l1": 8, "n_neurons_l2": 2}